{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Author : Elsie Rezinold Yedida\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPYeZDg3SFSL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n",
        "import tensorflow.keras.layers as tfl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6FKIrORSqvW",
        "outputId": "795e1d44-c347-4a77-94d3-a90ad9ba6415"
      },
      "outputs": [],
      "source": [
        "(train,test),info = tfds.load('horses_or_humans',split=\n",
        "\t\t['train','test'],\n",
        "\t\twith_info=True,\n",
        "\t\tas_supervised=True\n",
        "\t\t)\n",
        "\n",
        "print(info)\n",
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM_41pOUTRZ9"
      },
      "outputs": [],
      "source": [
        "def format_image(image,label):\n",
        "\timage = tf.image.resize(image,(100,100))/255.0\n",
        "\treturn image,label\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "num_examples = info.splits['train'].num_examples\n",
        "\n",
        "train_dataset = train.cache().shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_dataset = test.cache().map(format_image).batch(BATCH_SIZE).prefetch(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "CVU6ZpDYSxXR",
        "outputId": "0c68a31b-ed2c-47bd-d729-669a11aaadaf"
      },
      "outputs": [],
      "source": [
        "image,label = next(iter(train_dataset.take(1)))\n",
        "def plot(images):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img,ax in zip(images,axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot(image[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mm5pl-pYOSs",
        "outputId": "1fbb0e62-ecfe-476d-9d93-403bb90d32a2"
      },
      "outputs": [],
      "source": [
        "print(label[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Kco7I0aYMt"
      },
      "outputs": [],
      "source": [
        "\n",
        "model1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.8),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.8),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kerbkyVFauGj",
        "outputId": "c6b2afc8-8f2f-4e53-b41c-aef7952e577b"
      },
      "outputs": [],
      "source": [
        "model1.compile(loss='BinaryCrossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "EPOCHS_MODEL1 = EPOCHS\n",
        "\n",
        "history1 = model1.fit(\n",
        "      train_dataset,\n",
        "      epochs=EPOCHS_MODEL1,\n",
        "      verbose=1,\n",
        "      validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh5N0EyOhr2p"
      },
      "outputs": [],
      "source": [
        "\n",
        "def identity_block(X, f, filters, initializer=random_uniform):\n",
        "    F1, F2, F3 = filters\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(filters = F2, kernel_size = f,strides=(1,1),padding='same',kernel_initializer=initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(filters = F3, kernel_size = 1,strides=(1,1),padding='valid',kernel_initializer=initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X_shortcut,X])\n",
        "    X = Activation('relu')(X)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH2R4zpAilYD"
      },
      "outputs": [],
      "source": [
        "def convolutional_block(X, f, filters, s = 2, initializer=glorot_uniform):\n",
        "    F1, F2, F3 = filters\n",
        "    X_shortcut = X\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(filters = F2, kernel_size = f, strides = (1, 1), padding='same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sc5_MaSisxt"
      },
      "outputs": [],
      "source": [
        "train_dataset1 = train_dataset\n",
        "\n",
        "test_dataset1 = test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rVDexhvjRXG"
      },
      "outputs": [],
      "source": [
        "def RN(input_shape = (64, 64, 3), classes = 1):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X,f=3,filters=[128,128,512],s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X,f=3,filters=[256,256,1024],s=2)\n",
        "\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X,f=3,filters=[512,512,2048],s=2)\n",
        "\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    X = AveragePooling2D()(X)\n",
        "#    X = MaxPooling2D()(X)\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V96j6i2Sjhr4",
        "outputId": "4063058c-81b2-453e-f47c-45f8544af3b5"
      },
      "outputs": [],
      "source": [
        "model2 = RN(input_shape = (100,100, 3), classes = 1)\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "EPOCHS_MODEL2 = EPOCHS\n",
        "history2 = model2.fit(\n",
        "      train_dataset1,\n",
        "      epochs=EPOCHS_MODEL2,\n",
        "      validation_data=test_dataset1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "4CIAHPe9lgmA",
        "outputId": "9f122a48-88b5-4e0f-de97-08c5ca102351"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc = history1.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy']\n",
        "acc2 = history2.history['accuracy']\n",
        "val_acc2 = history2.history['val_accuracy']\n",
        "\n",
        "epochs_range1 = range(EPOCHS_MODEL1)\n",
        "epochs_range2 = range(EPOCHS_MODEL2)\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "loss2 = history2.history['loss']\n",
        "val_loss2 = history2.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(4,1,1)\n",
        "plt.plot(epochs_range1,acc,label='Training_accuracy for model1')\n",
        "plt.plot(epochs_range1,val_acc,label='validation_accuracy for model1')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "\n",
        "plt.subplot(4,1,2)\n",
        "plt.plot(epochs_range2,acc2,label='Training_accuracy for model2')\n",
        "plt.plot(epochs_range2,val_acc2,label='validation_accuracy for model2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "\n",
        "plt.subplot(4,1,3)\n",
        "plt.plot(epochs_range1,loss,label='loss for model1')\n",
        "plt.plot(epochs_range1,val_loss,label='validation_loss for model1')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training & Validation loss')\n",
        "\n",
        "plt.subplot(4,1,4)\n",
        "plt.plot(epochs_range2,loss2,label='loss for model2')\n",
        "plt.plot(epochs_range2,val_loss2,label='validation_loss for model2')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training & Validation loss')\n",
        "plt.savefig('./final_comparision_between models 1&2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqMhjt3I6F83"
      },
      "outputs": [],
      "source": [
        "def format_image(image,label):\n",
        "\timage = tf.image.resize(image,(160,160))#/255.0\n",
        "\treturn image,label\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "num_examples = info.splits['train'].num_examples\n",
        "train_dataset2 = train.cache().shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_dataset2 = test.cache().map(format_image).batch(BATCH_SIZE).prefetch(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVapHE3c7VKS"
      },
      "outputs": [],
      "source": [
        "def data_augmenter():\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip('horizontal'))\n",
        "    data_augmentation.add(RandomRotation(0.2))\n",
        "\n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSAQJe3y74fc"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRFiS4RP77jK"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (160,160)\n",
        "def horse_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
        "\n",
        "\n",
        "\n",
        "    input_shape = image_shape + (3,)\n",
        "\n",
        "\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
        "                                                   include_top=False,\n",
        "                                                   weights='imagenet') # From imageNet\n",
        "\n",
        "    # freeze the base model by making it non trainable\n",
        "    base_model.trainable = False\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # apply data augmentation to the inputs\n",
        "    x = data_augmentation(inputs)\n",
        "\n",
        "    # data preprocessing using the same weights the model was trained on\n",
        "    x = preprocess_input(x)\n",
        "\n",
        "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
        "    x = base_model(x, training=False)\n",
        "\n",
        "    # add the new Binary classification layers\n",
        "    # use global avg pooling to summarize the info in each channel\n",
        "    x = tfl.GlobalAveragePooling2D()(x)\n",
        "    # include dropout with probability of 0.2 to avoid overfitting\n",
        "    x = tfl.Dropout(0.2)(x)\n",
        "\n",
        "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
        "    outputs = tfl.Dense(1)(x)\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKz4e3Hd8ziV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model3 = horse_model(IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRrMz_pY-Be5"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.001\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVb2he2M-PLF",
        "outputId": "9e4c14a2-9924-4b27-e2c9-fae662da0851"
      },
      "outputs": [],
      "source": [
        "history3 = model3.fit(train_dataset2, epochs=20,batch_size=32,validation_data=test_dataset2,steps_per_epoch=33,validation_steps=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "83FndLrC-dBL",
        "outputId": "16b3fcce-bbe9-40a3-e757-6bd4d3587594"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc = history3.history['accuracy']\n",
        "val_acc = history3.history['val_accuracy']\n",
        "epochs_range = range(20)\n",
        "\n",
        "loss = history3.history['loss']\n",
        "val_loss = history3.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(epochs_range,acc,label='Training_accuracy for model3')\n",
        "plt.plot(epochs_range,val_acc,label='validation_accuracy for model3')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epochs_range,loss,label='loss for model3')\n",
        "plt.plot(epochs_range,val_loss,label='validation_loss for model3')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Training & Validation loss')\n",
        "plt.savefig('./models3')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgRD-zrHK7JG",
        "outputId": "ba021339-15ee-467b-d28c-bdc5b1de9ab7"
      },
      "outputs": [],
      "source": [
        "model1.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDDs8EykLFcm",
        "outputId": "5b305bad-9efe-40a0-cf1b-40e7bd755962"
      },
      "outputs": [],
      "source": [
        "model2.evaluate(test_dataset1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh3eLPvlLNkm",
        "outputId": "b35f2156-07bd-48f5-91df-e5e3f2af0b51"
      },
      "outputs": [],
      "source": [
        "model3.evaluate(test_dataset2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
